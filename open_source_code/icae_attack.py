import torch
import torch.nn.functional as F
from torch.optim import Adam
from tqdm import tqdm
from transformers import HfArgumentParser
from modeling_icae_multi_span import ICAE, ModelArguments, DataArguments, TrainingArguments
from peft import LoraConfig
from safetensors.torch import load_file
from utils import generate_output_from_attacked_suffix

class ICAEAttackerBase:
    def __init__(self, model, base_text, suffix_len=5, num_steps=1000, lr=1e-2, device="cuda"):
        parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))
        model_args, data_args, training_args = parser.parse_args_into_dataclasses()

        lora_config = LoraConfig(
            r=512,
            lora_alpha=32,
            lora_dropout=model_args.lora_dropout,
            bias="none",
            task_type="CAUSAL_LM"
        )

        self.model = ICAE(model_args, training_args, lora_config)
        state_dict = load_file(training_args.output_dir)
        self.model.load_state_dict(state_dict, strict=False)
        self.model = self.model.to(device)
        self.model.eval()
        self.tokenizer = self.model.tokenizer
        self.device = device
        self.suffix_len = suffix_len
        self.num_steps = num_steps
        self.lr = lr

        # Prepare base input and embeddings
        self.base_input_ids = self.tokenizer(base_text, truncation=True, max_length=5120, padding=False)['input_ids']
        self.base_input_ids = torch.LongTensor([self.base_input_ids]).to(device)
        with torch.no_grad():
            self.base_embeds = self.model.tokens_to_embeddings(self.base_input_ids)

        # Initialize trainable token logits
        vocab_size, _ = self.model.get_input_embeddings().weight.shape
        self.token_logits = torch.randn((suffix_len, vocab_size), requires_grad=True, device=device)
        self.optimizer = Adam([self.token_logits], lr=lr)

    def step(self):
        raise NotImplementedError("Must be implemented by subclasses.")

    def run(self):
        for step in tqdm(range(self.num_steps)):
            self.optimizer.zero_grad()
            loss = self.step()
            loss.backward()
            self.optimizer.step()

            if step % 10 == 0:
                print(f"[Step {step}] Loss: {loss.item():.4f}")
        return self.finalize()

    def finalize(self):
        with torch.no_grad():
            token_probs = F.softmax(self.token_logits, dim=-1)
            token_ids = torch.argmax(token_probs, dim=-1).tolist()
            decoded_suffix = self.tokenizer.decode(token_ids)
            print(f"\n[Final Decoded Suffix]: {decoded_suffix}")
            return decoded_suffix, token_ids
        
class TargetedICAEAttacker(ICAEAttackerBase):
    def __init__(self, model, base_text, target_text, suffix_len=5, num_steps=1000, lr=1e-2, device="cuda"):
        super().__init__(model, base_text, suffix_len, num_steps, lr, device)

        # Compute target embedding
        target_input_ids = self.tokenizer(target_text, truncation=True, max_length=5120, padding=False)['input_ids']
        target_input_ids = torch.LongTensor([target_input_ids]).to(device)
        with torch.no_grad():
            self.target_embedding = self.model._compress(target_input_ids).detach()

    def step(self):
        token_probs = F.softmax(self.token_logits, dim=-1).to(self.model.icae.dtype)
        soft_suffix = token_probs @ self.model.get_input_embeddings().weight
        combined = torch.cat([self.base_embeds, soft_suffix.unsqueeze(0)], dim=1)
        memory_slot = self.model._compress(inputs_embeds=combined)
        loss = -F.cosine_similarity(memory_slot, self.target_embedding.unsqueeze(0), dim=-1).mean()
        return loss
    
class NonTargetedICAEAttacker(ICAEAttackerBase):
    def __init__(self, model, base_text, suffix_len=5, num_steps=1000, lr=1e-2, device="cuda"):
        super().__init__(model, base_text, suffix_len, num_steps, lr, device)
        with torch.no_grad():
            self.base_embedding = self.model._compress(self.base_input_ids).detach()

    def step(self):
        token_probs = F.softmax(self.token_logits, dim=-1).to(self.model.icae.dtype)
        soft_suffix = token_probs @ self.model.get_input_embeddings().weight
        combined = torch.cat([self.base_embeds, soft_suffix.unsqueeze(0)], dim=1)
        memory_slot = self.model._compress(inputs_embeds=combined)
        loss = F.cosine_similarity(memory_slot, self.base_embedding, dim=-1).mean()
        return loss  # maximize distance from base == minimize similarity
    
class MinimalEditICAEAttacker(ICAEAttackerBase):
    def __init__(self, model, base_text, target_text=None, num_steps=1000, lr=1e-2, device="cuda", targeted=True, edit_weight=0.01):
        parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))
        model_args, data_args, training_args = parser.parse_args_into_dataclasses()

        lora_config = LoraConfig(
            r=512,
            lora_alpha=32,
            lora_dropout=model_args.lora_dropout,
            bias="none",
            task_type="CAUSAL_LM"
        )

        self.model = model
        self.model.eval()
        self.tokenizer = self.model.tokenizer
        self.device = device
        self.lr = lr
        self.num_steps = num_steps
        self.edit_weight = edit_weight
        self.targeted = targeted

        self.base_input_ids = self.tokenizer(base_text, truncation=True, max_length=5120, padding=False)['input_ids']
        self.base_input_ids = torch.LongTensor([self.base_input_ids]).to(device)
        self.base_len = self.base_input_ids.shape[1]

        with torch.no_grad():
            self.original_embeds = self.model.tokens_to_embeddings(self.base_input_ids)[0]  # [L, D]
            if targeted:
                assert target_text is not None, "Targeted attack requires a target_text."
                target_ids = self.tokenizer(target_text, truncation=True, max_length=5120, padding=False)['input_ids']
                target_ids = torch.LongTensor([target_ids]).to(device)
                self.target_embedding = self.model._compress(target_ids).detach()
            else:
                self.base_embedding = self.model._compress(self.base_input_ids).detach()

        vocab_size = self.model.get_input_embeddings().weight.shape[0]
        self.token_logits = torch.zeros((self.base_len, vocab_size), requires_grad=True, device=device)
        self.optimizer = Adam([self.token_logits], lr=lr)

    def step(self):
        token_probs = F.softmax(self.token_logits, dim=-1).to(self.model.icae.dtype)  # [L, V]
        soft_embeds = token_probs @ self.model.get_input_embeddings().weight  # [L, D]
        soft_embeds = soft_embeds.unsqueeze(0)  # [1, L, D]

        memory = self.model._compress(inputs_embeds=soft_embeds)

        if self.targeted:
            attack_loss = -F.cosine_similarity(memory, self.target_embedding.unsqueeze(0), dim=-1).mean()
        else:
            attack_loss = F.cosine_similarity(memory, self.base_embedding.unsqueeze(0), dim=-1).mean()

        # Edit loss: Encourage one-hot close to the original token
        original_ids = self.base_input_ids[0]
        original_onehot = F.one_hot(original_ids, num_classes=token_probs.shape[-1]).float()
        edit_loss = F.kl_div(token_probs.log(), original_onehot, reduction='batchmean')

        total_loss = attack_loss + self.edit_weight * edit_loss
        return total_loss

    def finalize(self):
        with torch.no_grad():
            token_probs = F.softmax(self.token_logits, dim=-1)
            token_ids = torch.argmax(token_probs, dim=-1).tolist()
            decoded_text = self.tokenizer.decode(token_ids)
            print(f"\n[Final Edited Text]: {decoded_text}")
            return decoded_text, token_ids
        
if __name__ == "__main__":
    device = "cuda"

    # Load model and arguments
    parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    lora_config = LoraConfig(
        r=512,
        lora_alpha=32,
        lora_dropout=model_args.lora_dropout,
        bias="none",
        task_type="CAUSAL_LM"
    )

    model = ICAE(model_args, training_args, lora_config)
    state_dict = load_file(training_args.output_dir)
    model.load_state_dict(state_dict, strict=False)
    model = model.to(device)

    base_text = "Google Pixel 7a \u2013 The Pixel 7a combines Google\u2019s signature computational photography with a clean, bloat-free Android 14 experience. Its Tensor G2 chip, paired with 6 GB of RAM, handles everything from multitasking to light gaming without hiccups, and the 6.1\u2033 FHD+ OLED display offers superb contrast and color accuracy. You\u2019ll get at least three years of OS upgrades and five years of security patches straight from Google, ensuring your phone stays current. The 64 MP main sensor and 13 MP ultrawide lens\u2014backed by Night Sight and HDR+\u2014deliver crisp, vibrant shots in most lighting conditions, and wireless charging support rounds out the package."
    target_text = "Samsung Galaxy A54 \u2013 Samsung\u2019s A-series flagship for under $500, the Galaxy A54 sports a 6.4\u2033 Super AMOLED display with 120 Hz refresh rate, making scrolling and games feel buttery smooth. Under the hood, the Exynos 1380 chipset and 6 GB of RAM power through social apps, streaming, and productivity tasks with ease, while the 5,000 mAh battery paired with 25 W USB-C charging easily lasts a full day. Samsung promises four years of security updates and three major OS upgrades\u2014rare at this price\u2014and the triple-camera array (50 MP main, 12 MP ultrawide, 5 MP macro) captures detailed photos in daylight and acceptable night-mode results when combined with Samsung\u2019s scene optimizer."
    requirements = "1. Budget under $500; 2. Good all-round performance; 3. Reliable software updates; 4. Decent camera quality."
    #  Option 1: Targeted attack
    # attacker = TargetedICAEAttacker(model, base_text, target_text, suffix_len=5, num_steps=50, device=device)

    #  Option 2: Non-targeted attack
    # attacker = NonTargetedICAEAttacker(model, base_text, suffix_len=5, num_steps=50, device=device)

    #  Option 3: Minimal edit attack
    # attacker = MinimalEditICAEAttacker(model, base_text, target_text=target_text, num_steps=50, device=device, targeted=True)
    attacker = MinimalEditICAEAttacker(model, base_text, target_text=None, num_steps=50, device=device, targeted=False)


    edited_text, token_ids = attacker.run()

    generated_answer = generate_output_from_attacked_suffix(
        model,
        base_text=base_text,
        suffix_token_ids=token_ids,
        prompt_text="I want to buy a phone and I have some requirements, please recommend a phone for me. My requirements are: " + requirements,
        max_out_length=128,
        device="cuda"
    )

    print("\n[Final Answer]:", generated_answer)